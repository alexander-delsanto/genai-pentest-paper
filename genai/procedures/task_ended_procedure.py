from pydantic import BaseModel
from typing import Dict, Any, Type
from .base_procedure import BaseProcedure


class TaskEndedModel(BaseModel):
    task_ended: bool

    class Config:
        @staticmethod
        def json_schema_extra(schema: Dict[str, Any], model: Type['TaskEndedModel']) -> None:
            for prop in schema.get('properties', {}).values():
                prop.pop('title', None)


class TaskEndedProcedure(BaseProcedure):
    """A reasoning procedure that invokes the LLM to decide if the current task
    or sub-task is ended or not.

    This class extends the BaseProcedure to handle reasoning. 
    It leverages the agent scratchpad and the current task or sub-task and 
    asks the LLM to decide if the task is ended or not.

    Args:
        llm (LLMClient): The LLM client responsible for executing tasks based 
            on the prompt.
        prompt_template (str): The prompt template that will be formatted and 
            used as input to the LLM.

    Attributes:
        llm (LLMClient): The LLM client responsible for executing tasks based 
            on the prompt.
        prompt_template (str): The prompt template that will be formatted and 
            used as input to the LLM.

    Methods:
        run(task, history): Generates actions based on the given inputs using the LLM.
    """

    def run(self, task: str, history: list):
        """Execute the task ended procedure based on the current task and 
        agent scratchpad

        Args:
            task (str): the current description of the task
            history (list):  agent scratchpad formatted as messages list

        Returns:
            TaskEndedModel: The TaskEndedModel formatted by the LLM
        """
        # Format the prompt
        prompt = self.prompt_template.format(
            task=task,
            history=history
        )

        # Invoke LLM
        llm_out = self.llm.invoke(
            response_model=TaskEndedModel,
            system_prompt=prompt,
            messages=[]
        )

        return llm_out
